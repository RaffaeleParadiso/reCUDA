{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pycuda\n",
    "import pycuda\n",
    "!nvcc --version  # Check CUDA version\n",
    "!gcc --version # Check gcc version\n",
    "!nvidia-smi # Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.driver as drv\n",
    "drv.init()\n",
    "drv.get_version()\n",
    "devn=drv.Device.count()\n",
    "print(\"N GPU \"+str(devn))\n",
    "devices = []\n",
    "for i in range(devn):\n",
    "  devices.append(drv.Device(i))\n",
    "for sp in devices:\n",
    "  print(\"GPU name: \"+str(sp.name))\n",
    "  print(\"Compute Capability = \"+str(sp.compute_capability()))\n",
    "  print(\"Total Memory = \"+str(sp.total_memory()/(2.**20))+' MBytes')\n",
    "  attr = sp.get_attributes()\n",
    "  print(attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With DeviceData()\n",
    "from pycuda import autoinit\n",
    "from pycuda.tools import DeviceData\n",
    "specs = DeviceData()\n",
    "print ('Max threads per block = '+str(specs.max_threads))\n",
    "print ('Warp size             = '+str(specs.warp_size))\n",
    "print ('Warps per MP          = '+str(specs.warps_per_mp))\n",
    "print ('Thread Blocks per MP  = '+str(specs.thread_blocks_per_mp))\n",
    "print ('Registers             = '+str(specs.registers))\n",
    "print ('Shared memory         = '+str(specs.shared_memory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example GPU in C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile VecAdd.cu\n",
    "# include <stdio.h>\n",
    "# include <cuda_runtime.h>\n",
    "// CUDA Kernel\n",
    "__global__ void vectorAdd(const float *A, const float *B, float *C, int numElements)\n",
    "{\n",
    "    int i = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    if (i < numElements)\n",
    "    {\n",
    "        C[i] = A[i] + B[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "/**\n",
    " * Host main routine\n",
    " */\n",
    "int main(void)\n",
    "{\n",
    "    int numElements = 15;\n",
    "    size_t size = numElements * sizeof(float);\n",
    "    printf(\"[Vector addition of %d elements]\\n\", numElements);\n",
    "\n",
    "    float a[numElements],b[numElements],c[numElements];\n",
    "    float *a_gpu,*b_gpu,*c_gpu;\n",
    "\n",
    "    cudaMalloc((void **)&a_gpu, size);\n",
    "    cudaMalloc((void **)&b_gpu, size);\n",
    "    cudaMalloc((void **)&c_gpu, size);\n",
    "\n",
    "    for (int i=0;i<numElements;++i ){\n",
    "\n",
    "        a[i] = i*i;\n",
    "        b[i] = i;\n",
    "\n",
    "    }\n",
    "    // Copy the host input vectors A and B in host memory to the device input vectors in\n",
    "    // device memory\n",
    "    printf(\"Copy input data from the host memory to the CUDA device\\n\");\n",
    "    cudaMemcpy(a_gpu, a, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(b_gpu, b, size, cudaMemcpyHostToDevice);\n",
    "\n",
    "    // Launch the Vector Add CUDA Kernel\n",
    "    int threadsPerBlock = 256;\n",
    "    int blocksPerGrid =(numElements + threadsPerBlock - 1) / threadsPerBlock;\n",
    "    printf(\"CUDA kernel launch with %d blocks of %d threads\\n\", blocksPerGrid, threadsPerBlock);\n",
    "    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(a_gpu, b_gpu, c_gpu, numElements);\n",
    "\n",
    "    // Copy the device result vector in device memory to the host result vector\n",
    "    // in host memory.\n",
    "    printf(\"Copy output data from the CUDA device to the host memory\\n\");\n",
    "    cudaMemcpy(c, c_gpu, size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    for (int i=0;i<numElements;++i ){\n",
    "        printf(\"%f \\n\",c[i]);\n",
    "    }\n",
    "\n",
    "    // Free device global memory\n",
    "    cudaFree(a_gpu);\n",
    "    cudaFree(b_gpu);\n",
    "    cudaFree(c_gpu);\n",
    "\n",
    "    printf(\"Done\\n\");\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o VecAdd VecAdd.cu\n",
    "!./VecAdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyCUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycuda import autoinit\n",
    "from pycuda import gpuarray\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a,b,c vector on host. All of them with length 15, a with numbers from 0..14 and b with squares. c is initialized to 0\n",
    "aux = range(15)\n",
    "a = np.array(aux).astype(np.float32)\n",
    "b = (a*a).astype(np.float32)\n",
    "c = np.zeros(len(aux)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector on GPU, copy from host to device\n",
    "a_gpu = gpuarray.to_gpu(a)\n",
    "b_gpu = gpuarray.to_gpu(b)\n",
    "c_gpu = gpuarray.to_gpu(c)\n",
    "# Sum two vectors using CUDA\n",
    "c_gpu=a_gpu+b_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c_gpu)\n",
    "c_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to sum, is to use the elementwise method, which applies the same \"Operation\" to all the elements of the vectors\n",
    "\n",
    "from pycuda.elementwise import ElementwiseKernel\n",
    "myCudaFunc = ElementwiseKernel(arguments = \"float *a, float *b, float *c\",\n",
    "                               operation = \"c[i] = a[i]+b[i]\",\n",
    "                               name = \"mySumK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myCudaFunc(a_gpu,b_gpu,c_gpu)\n",
    "c_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycuda.elementwise import ElementwiseKernel\n",
    "lin_comb = ElementwiseKernel(\n",
    "        \"float a, float *x, float b, float *y, float *z\",\n",
    "        \"z[i] = a*x[i] + b*y[i]\",\n",
    "        \"linear_combination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_comb(3.,a_gpu,5.,b_gpu,c_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to write a kernel is to use the SourceModule method. This method allows to write more complex kernels. The idea is that these kernels are written in Cuda/C\n",
    "from pycuda.compiler import SourceModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudaCode = open(\"VecAdd.cu\",\"r\")\n",
    "myCUDACode = cudaCode.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the code just-in-time with the SourceModule() method\n",
    "myCode = SourceModule(myCUDACode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importedKernel = myCode.get_function(\"vectorAdd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU geometry\n",
    "nThreadsPerBlock = 256\n",
    "nBlockPerGrid = 1\n",
    "nGridsPerBlock = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the GPU device\n",
    "c_gpu.set(c)\n",
    "c_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pointers in gpu memory is given by gpudata\n",
    "a_gpu.gpudata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_gpu.gpudata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch the imported kernel passing the pointers to the vectors and the GPU geometry\n",
    "importedKernel(a_gpu.gpudata, b_gpu.gpudata, c_gpu.gpudata, block=(nThreadsPerBlock,nBlockPerGrid,nGridsPerBlock))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_gpu"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
